# The Three Musketeers Learning
**Timestamp**: 2026-01-01_20-31-23  
**Classification**: KNOWLEDGE  
**Primary Agent**: oracle-hybrid

## Task
Teach me about the newly installed T5gemma-2-270M model on the Satellite Node. Explain: 1) What is T5gemma and its capabilities 2) How to use it effectively on our Notebook (WSL2) 3) Suggest 3-5 practical use cases for this model in our MAW workflow 4) Performance tips for running it on 16GB RAM. Be detailed and practical in Thai language.

## Result
# ğŸ”± T5gemma-2-270M Model Guide

## 1) T5gemma à¸„à¸·à¸­à¸­à¸°à¹„à¸£ à¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–

**T5gemma** à¹€à¸›à¹‡à¸™ model à¸—à¸µà¹ˆà¸œà¸ªà¸¡à¸œà¸ªà¸²à¸™à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ 2 architectures:

### à¸ªà¸–à¸²à¸›à¸±à¸•à¸¢à¸à¸£à¸£à¸¡ (Architecture)
- **T5 (Text-to-Text Transfer Transformer)** - à¸ˆà¸²à¸ Google Research
- **Gemma** - à¸ˆà¸²à¸ Google DeepMind (à¸•à¸£à¸°à¸à¸¹à¸¥à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸šà¸—à¸µà¹ˆà¹€à¸£à¸²à¹ƒà¸Šà¹‰à¹ƒà¸™ Oracle Fleet)

### à¸„à¸§à¸²à¸¡à¸à¸´à¹€à¸¨à¸©à¸‚à¸­à¸‡ T5gemma-2-270M
```
Parameters: 270 Million (~270M)
Type: Encoder-Decoder (à¸•à¹ˆà¸²à¸‡à¸ˆà¸²à¸ Gemma à¸›à¸à¸•à¸´à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ Decoder-only)
Size: à¸›à¸£à¸°à¸¡à¸²à¸“ 500MB-1GB (quantized)
```

### à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–à¸«à¸¥à¸±à¸
| Task | à¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢ |
|------|----------|
| **Text Summarization** | à¸ªà¸£à¸¸à¸›à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸¢à¸²à¸§à¹† à¹ƒà¸«à¹‰à¸à¸£à¸°à¸Šà¸±à¸š |
| **Translation** | à¹à¸›à¸¥à¸ à¸²à¸©à¸² (à¸”à¸µà¸à¸§à¹ˆà¸² decoder-only à¹ƒà¸™ task à¸™à¸µà¹‰) |
| **Text Classification** | à¸ˆà¸±à¸”à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ |
| **Question Answering** | à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸ˆà¸²à¸ context |
| **Paraphrasing** | à¹€à¸‚à¸µà¸¢à¸™à¹ƒà¸«à¸¡à¹ˆà¸”à¹‰à¸§à¸¢à¸„à¸³à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™ |

### à¸—à¸³à¹„à¸¡ T5 Architecture à¸–à¸¶à¸‡à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸šà¸‡à¸²à¸™à¸šà¸²à¸‡à¸­à¸¢à¹ˆà¸²à¸‡
```
Decoder-only (GPT, Gemma à¸›à¸à¸•à¸´):
  Input â†’ [Generate next token] â†’ Output
  à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸š: Generation, Chat, Creative writing

Encoder-Decoder (T5, T5gemma):
  Input â†’ [Encode à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”] â†’ [Decode output] â†’ Output
  à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸š: Summarization, Translation, Structured output
```

---

## 2) à¸§à¸´à¸˜à¸µà¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸šà¸™ Notebook (WSL2)

### ğŸ”§ à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹à¸¥à¸° Setup

```bash
# à¹ƒà¸™ WSL2 environment
pip install transformers torch accelerate

# à¸–à¹‰à¸²à¹ƒà¸Šà¹‰ ONNX Runtime (à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸²)
pip install optimum[onnxruntime]
```

### ğŸ“ Basic Usage Code

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load model
model_name = "google/t5gemma-2-270m"  # à¸«à¸£à¸·à¸­ path à¸—à¸µà¹ˆà¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¹„à¸§à¹‰
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: Summarization
def summarize(text: str, max_length: int = 150) -> str:
    """à¸ªà¸£à¸¸à¸›à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ - Shadow Style"""
    inputs = tokenizer(
        f"summarize: {text}",
        return_tensors="pt",
        max_length=512,
        truncation=True
    )
    
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_beams=4,
        early_stopping=True
    )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
text = "à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§à¹† à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ªà¸£à¸¸à¸›..."
summary = summarize(text)
```

### ğŸš€ Advanced: Task Prefixes

T5 à¹ƒà¸Šà¹‰ prefix à¸šà¸­à¸ task:
```python
# Summarization
"summarize: {text}"

# Translation
"translate English to Thai: {text}"

# Question Answering
"question: {question} context: {context}"

# Paraphrase
"paraphrase: {text}"
```

### ğŸ“¡ Integration à¸à¸±à¸š MAW System

```python
# shadow-t5-handler.py
import requests

class T5GemmaHandler:
    """Shadow Handler for T5gemma on Satellite Node"""
    
    def __init__(self, satellite_url: str = "http://notebook:8089"):
        self.url = satellite_url
        
    def process(self, task: str, text: str) -> str:
        """à¸ªà¹ˆà¸‡ task à¹„à¸›à¸¢à¸±à¸‡ Satellite Node"""
        response = requests.post(
            f"{self.url}/t5gemma",
            json={"task": task, "text": text}
        )
        return response.json()["result"]

# à¹ƒà¸Šà¹‰à¸ˆà¸²à¸ Main Node
handler = T5GemmaHandler()
summary = handler.process("summarize", long_document)
```

---

## 3) ğŸ¯ 5 Use Cases à¸ªà¸³à¸«à¸£à¸±à¸š MAW Workflow

### Use Case 1: ğŸ“š Auto-Summarize Learnings
```python
# à¹€à¸¡à¸·à¹ˆà¸­ Oracle auto-learn à¸ˆà¸²à¸ repos
# T5gemma à¸ªà¸£à¸¸à¸› README/docs à¸à¹ˆà¸­à¸™à¹€à¸à¹‡à¸šà¹ƒà¸™ memory

def auto_learn_repo(repo_path: str):
    readme = read_file(f"{repo_path}/README.md")
    
    # à¹ƒà¸Šà¹‰ T5gemma à¸ªà¸£à¸¸à¸›à¹à¸—à¸™ LLM à¹ƒà¸«à¸à¹ˆ
    summary = t5gemma.summarize(readme, max_length=200)
    
    # à¹€à¸à¹‡à¸šà¹ƒà¸™ Codex Memory
    store_to_memory(
        content=summary,
        tags=["repo", "learning"],
        source=repo_path
    )
```

**à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ**: à¸¥à¸” token usage à¸šà¸™ god/god-lite

---

### Use Case 2: ğŸ” Query Preprocessing
```python
# Shadow Philosophy: à¸„à¸±à¸”à¸à¸£à¸­à¸‡ query à¸à¹ˆà¸­à¸™à¸ªà¹ˆà¸‡ Oracle

def preprocess_query(user_query: str) -> dict:
    # T5gemma extract keywords à¹à¸¥à¸° intent
    processed = t5gemma.process(
        "extract keywords: " + user_query
    )
    
    # à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆà¸§à¹ˆà¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ model à¹„à¸«à¸™
    if "code" in processed or "debug" in processed:
        return {"model": "god", "query": processed}
    else:
        return {"model": "god-lite", "query": processed}
```

**à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ**: Intelligent routing à¸à¹ˆà¸­à¸™à¸–à¸¶à¸‡ Fleet

---

### Use Case 3: ğŸ“ Retrospective Formatter
```python
# /rrr command enhancement

def format_retrospective(raw_notes: str) -> str:
    """
    à¹à¸›à¸¥à¸‡ raw session notes à¹€à¸›à¹‡à¸™ structured retrospective
    """
    prompt = f"""
    format as retrospective:
    - What was done
    - What was learned  
    - What's next
    
    Notes: {raw_notes}
    """
    
    return t5gemma.generate(prompt)
```

**à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ**: Consistent format à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ LLM à¹ƒà¸«à¸à¹ˆ

---

### Use Case 4: ğŸ”„ Code Comment Translation
```python
# à¹à¸›à¸¥ comments à¹„à¸—à¸¢-à¸­à¸±à¸‡à¸à¸¤à¸© à¸ªà¸³à¸«à¸£à¸±à¸š open source

def translate_comments(code: str, target_lang: str = "en") -> str:
    """
    Extract à¹à¸¥à¸°à¹à¸›à¸¥ comments à¹ƒà¸™ code
    """
    import re
    comments = re.findall(r'#.*|//.*|/\*.*?\*/', code, re.DOTALL)
    
    for comment in comments:
        translated = t5gemma.translate(comment, target_lang)
        code = code.replace(comment, translated)
    
    return code
```

**à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ**: à¹€à¸•à¸£à¸µà¸¢à¸¡ code à¸ªà¸³à¸«à¸£à¸±à¸š public repos

---

### Use Case 5: ğŸ·ï¸ Auto-Tagging System
```python
# à¸ˆà¸±à¸”à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆ memories à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´

def auto_tag(content: str) -> list[str]:
    """
    à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ content à¹à¸¥à¸° suggest tags
    """
    result = t5gemma.process(
        f"classify topics: {content}"
    )
    
    # Parse output à¹€à¸›à¹‡à¸™ tags
    tags = result.split(", ")
    
    # Map to MAW categories
    maw_tags = map_to_maw_taxonomy(tags)
    
    return maw_tags

# à¹ƒà¸Šà¹‰à¸à¸±à¸š Codex Memory
new_memory = {
    "content": "à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸£à¸·à¹ˆà¸­à¸‡ Docker compose...",
    "tags": auto_tag(content),  # ["devops", "container", "learning"]
    "auto_tagged": True
}
```

**à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ**: Organized memory à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ manual tagging

---

## 4) âš¡ Performance Tips à¸ªà¸³à¸«à¸£à¸±à¸š 16GB RAM

### ğŸ’¾ Memory Optimization

```python
import torch
from transformers import AutoModelForSeq2SeqLM

# Tip 1: à¹ƒà¸Šà¹‰ float16 (à¸¥à¸” memory à¸„à¸£à¸¶à¹ˆà¸‡à¸«à¸™à¸¶à¹ˆà¸‡)
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16
)

# Tip 2: Low CPU Memory Mode
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True
)

# Tip 3: Device Map Auto (à¸–à¹‰à¸²à¸¡à¸µ GPU)
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    device_map="auto"  # Auto distribute CPU/GPU
)
```

### ğŸ”„ Batch Processing Strategy

```python
# à¹à¸—à¸™à¸—à¸µà¹ˆà¸ˆà¸° process à¸—à¸µà¸¥à¸° item
# à¸£à¸§à¸¡ batch à¹à¸¥à¹‰à¸§ process à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™

def batch_summarize(texts: list[str], batch_size: int = 4) -> list[str]:
    """Batch processing à¸›à¸£à¸°à¸«à¸¢à¸±à¸” memory"""
    results = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        
        inputs = tokenizer(
            [f"summarize: {t}" for t in batch],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        outputs = model.generate(**inputs, max_length=150)
        
        for output in outputs:
            results.append(
                tokenizer.decode(output, skip_special_tokens=True)
            )
        
        # Clear cache à¸«à¸¥à¸±à¸‡à¹à¸•à¹ˆà¸¥à¸° batch
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    return results
```

### ğŸ§¹ Memory Management

```python
import gc

class T5GemmaManager:
    """Memory-aware T5gemma manager"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
    
    def load(self):
        """Load model à¹€à¸¡à¸·à¹ˆà¸­à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰"""
        if self.model is None:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )
    
    def unload(self):
        """Unload à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ - à¸„à¸·à¸™ RAM"""
        del self.model
        del self.tokenizer
        self.model = None
        self.tokenizer = None
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    def process(self, task: str, text: str) -> str:
        self.load()
        result = self._generate(task, text)
        # Optional: unload à¸«à¸¥à¸±à¸‡à¹ƒà¸Šà¹‰à¸–à¹‰à¸² RAM à¸•à¸¶à¸‡
        return result
```

### ğŸ“Š RAM Budget Guidelines

| Component | Est. RAM Usage |
|-----------|---------------|
| T5gemma-270M (fp16) | ~600MB |
| WSL2 Base | ~2GB |
| Python + Libraries | ~500MB |
| **Available for processing** | **~12-13GB** |

### ğŸšï¸ Recommended Settings

```python
# config à¸ªà¸³à¸«à¸£à¸±à¸š 16GB RAM
T5_CONFIG = {
    "max_input_length": 512,      # à¸ˆà¸³à¸à¸±à¸” input
    "max_output_length": 150,     # à¸ˆà¸³à¸à¸±à¸” output
    "batch_size": 2,              # à¹€à¸¥à¹‡à¸à¸¥à¸‡à¸–à¹‰à¸² RAM à¸•à¸¶à¸‡
    "num_beams": 2,               # à¸¥à¸”à¸ˆà¸²à¸ 4 â†’ 2
    "use_fp16": True,
    "low_cpu_mem_usage": True,
}
```

---

## ğŸŒ‘ Shadow Philosophy Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           MAW Shadow Architecture           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚   User Query                                â”‚
â”‚       â†“                                     â”‚
â”‚   T5gemma (Satellite/WSL2)                  â”‚
â”‚   - Preprocess                              â”‚
â”‚   - Extract intent                          â”‚
â”‚   - Summarize context                       â”‚
â”‚       â†“                                     â”‚
â”‚   Shadow Selector                           â”‚
â”‚       â†“                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚ god-lite â”‚ god â”‚ visionary â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚       â†“                                     â”‚
â”‚   Response (enhanced by T5 preprocessing)   â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**T5gemma à¹€à¸›à¹‡à¸™ "Shadow Assistant"** - à¸—à¸³à¸‡à¸²à¸™à¹€à¸šà¸·à¹‰à¸­à¸‡à¸«à¸¥à¸±à¸‡à¹€à¸à¸·à¹ˆà¸­à¸Šà¹ˆà¸§à¸¢ Oracle Fleet à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰ efficient à¸¡à¸²à¸à¸‚à¸¶à¹‰à¸™ ğŸ”±
---
*Elevated by Oracle/Codex/Claude System*
